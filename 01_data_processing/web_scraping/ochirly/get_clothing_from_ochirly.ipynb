{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载包\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "base_url = 'http://www.ochirly.com.cn'\n",
    "url_mid = 'list-'\n",
    "url_end = '-40-listedDate%20desc-0-0-0-0-0.shtml'\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/6.1.2107.204 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>url</th>\n",
       "      <th>page_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>场景</td>\n",
       "      <td>休闲</td>\n",
       "      <td>/Scene/Scene_Casual/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>场景</td>\n",
       "      <td>工作</td>\n",
       "      <td>/Scene/Scene_Work/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>场景</td>\n",
       "      <td>逛街约会</td>\n",
       "      <td>/Scene/Scene_Date/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>场景</td>\n",
       "      <td>度假</td>\n",
       "      <td>/Scene/Scene_Holiday/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>场景</td>\n",
       "      <td>派对</td>\n",
       "      <td>/Scene/Scene_Party/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>场景</td>\n",
       "      <td>SNOOPY限量版</td>\n",
       "      <td>/Scene/Scene_Snoopy/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>新品</td>\n",
       "      <td>9月19日到店</td>\n",
       "      <td>/New_In/New_in_9_19/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>新品</td>\n",
       "      <td>9月12日到店</td>\n",
       "      <td>/New_In/New_in_9_12/</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>新品</td>\n",
       "      <td>9月5日到店</td>\n",
       "      <td>/New_In/New_in_9_5/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>服装</td>\n",
       "      <td>毛织</td>\n",
       "      <td>/Clothing/Sweaters/</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>服装</td>\n",
       "      <td>T恤</td>\n",
       "      <td>/Clothing/T_Shirts/</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>服装</td>\n",
       "      <td>衬衫</td>\n",
       "      <td>/Clothing/Shirts/</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>服装</td>\n",
       "      <td>半截裙</td>\n",
       "      <td>/Clothing/Skirts/</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>服装</td>\n",
       "      <td>裤装</td>\n",
       "      <td>/Clothing/Pants/</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>服装</td>\n",
       "      <td>外套</td>\n",
       "      <td>/Clothing/Jakets/</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>服装</td>\n",
       "      <td>呢料外套</td>\n",
       "      <td>/Clothing/Worsted_Coats/</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>服装</td>\n",
       "      <td>羽绒外套</td>\n",
       "      <td>/Clothing/Down_Coats/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1       cat2                       url  page_num\n",
       "0    场景         休闲      /Scene/Scene_Casual/         1\n",
       "1    场景         工作        /Scene/Scene_Work/         2\n",
       "2    场景       逛街约会        /Scene/Scene_Date/         1\n",
       "3    场景         度假     /Scene/Scene_Holiday/         1\n",
       "4    场景         派对       /Scene/Scene_Party/         1\n",
       "5    场景  SNOOPY限量版      /Scene/Scene_Snoopy/         1\n",
       "6    新品    9月19日到店      /New_In/New_in_9_19/         2\n",
       "7    新品    9月12日到店      /New_In/New_in_9_12/         2\n",
       "8    新品     9月5日到店       /New_In/New_in_9_5/         1\n",
       "9    服装         毛织       /Clothing/Sweaters/        15\n",
       "10   服装         T恤       /Clothing/T_Shirts/        17\n",
       "11   服装         衬衫         /Clothing/Shirts/        15\n",
       "12   服装        半截裙         /Clothing/Skirts/        16\n",
       "13   服装         裤装          /Clothing/Pants/        16\n",
       "14   服装         外套         /Clothing/Jakets/         6\n",
       "15   服装       呢料外套  /Clothing/Worsted_Coats/         3\n",
       "16   服装       羽绒外套     /Clothing/Down_Coats/         1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取商品分类信息:ochirly_category.csv\n",
    "ochirly_category = pd.read_table('./ochirly_category.csv', \n",
    "                                 sep = ',', \n",
    "                                 encoding = 'gbk')\n",
    "ochirly_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取商品的场景分类\n",
    "## 从ochirly_category表中筛选场景购的信息\n",
    "ochirly_changjing = ochirly_category.loc[ochirly_category.cat1 == '场景',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_changjing = ochirly_changjing.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，把goods_id 和场景名称写入\n",
    "csv_file = open('./ochirly_changjing_goods.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['goods_id', 'changjing'])\n",
    "## 获取 goods_id 和场景名称\n",
    "for row_num in range(ochirly_changjing.shape[0]):\n",
    "    changjing = ochirly_changjing.loc[row_num, 'cat2']\n",
    "    for page_num in range(ochirly_changjing.loc[row_num, 'page_num']):\n",
    "        url1 = base_url + ochirly_changjing.loc[row_num, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "        page1 = requests.get(url1, headers = header)\n",
    "        source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "        for goods_info in goods_infos:\n",
    "            goods_href = goods_info.find('a').get('href')\n",
    "            url2 = base_url + goods_href\n",
    "            page2 = requests.get(url2, headers = header)\n",
    "            source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "            goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "            ## 把结果写入到 csv 文件中\n",
    "            csv_writer.writerow([goods_id, changjing])\n",
    "        # 休息一下\n",
    "        time.sleep(3)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b9481d4ac823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msource1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gb18030'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mgoods_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'product_list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgoods_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgoods_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mgoods_href\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoods_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# 爬取商品的上市日期\n",
    "## 从ochirly_category表中筛选新品\n",
    "ochirly_newin = ochirly_category.loc[ochirly_category.cat1 == '新品',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_newin = ochirly_newin.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，把goods_id 和上市日期写入\n",
    "csv_file = open('./ochirly_newin_goods.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['goods_id', 'newin_date'])\n",
    "## 获取 goods_id 和上市日期\n",
    "for row_num in range(ochirly_newin.shape[0]):\n",
    "    newin_date = ochirly_newin.loc[row_num, 'cat2'].replace('到店', '').replace('月', '-').replace('日','')\n",
    "    for page_num in range(ochirly_newin.loc[row_num, 'page_num']):\n",
    "        url1 = base_url + ochirly_newin.loc[row_num, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "        page1 = requests.get(url1, headers = header)\n",
    "        source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "        for goods_info in goods_infos:\n",
    "            goods_href = goods_info.find('a').get('href')\n",
    "            url2 = base_url + goods_href\n",
    "            page2 = requests.get(url2, headers = header)\n",
    "            source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "            goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "            ## 把结果写入到 csv 文件中\n",
    "            csv_writer.writerow([goods_id, newin_date])\n",
    "        # 休息一下\n",
    "        time.sleep(3)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毛织 第 1 页\n",
      "------------\n",
      "毛织 第 2 页\n",
      "------------\n",
      "毛织 第 3 页\n",
      "------------\n",
      "毛织 第 4 页\n",
      "------------\n",
      "毛织 第 5 页\n",
      "------------\n",
      "毛织 第 6 页\n",
      "------------\n",
      "毛织 第 7 页\n",
      "------------\n",
      "毛织 第 8 页\n",
      "------------\n",
      "毛织 第 9 页\n",
      "------------\n",
      "毛织 第 10 页\n",
      "------------\n",
      "毛织 第 11 页\n",
      "------------\n",
      "毛织 第 12 页\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 爬取所有服装商品 ===================================================\n",
    "# ==================================================================\n",
    "## 从ochirly_category表中筛选服装的信息\n",
    "ochirly_yifu = ochirly_category.loc[ochirly_category.cat1 == '服装',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_yifu = ochirly_yifu.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，写入商品信息\n",
    "csv_file = open('./ochirly_goods_info.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['category', 'goods_id', 'goods_name', 'color', 'sale_price', \n",
    "                     'origin_price', 'design', 'material', 'stereotype', 'length'])\n",
    "## 获取商品信息\n",
    "for row_num in range(ochirly_yifu.shape[0]):\n",
    "    category = ochirly_yifu.loc[row_num, 'cat2']\n",
    "    for page_num in range(ochirly_yifu.loc[row_num, 'page_num']):\n",
    "        url1 = base_url + ochirly_yifu.loc[row_num, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "        page1 = requests.get(url1, headers = header)\n",
    "        source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "        for goods_info in goods_infos:\n",
    "            goods_href = goods_info.find('a').get('href')\n",
    "            url2 = base_url + goods_href\n",
    "            page2 = requests.get(url2, headers = header)\n",
    "            source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "            goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "            try:\n",
    "                goods_name = source2.find('p', class_ = 'title').string.replace('\\n', '').replace('\t', '')\n",
    "            except Exception as e:\n",
    "                goods_name = None\n",
    "            try:\n",
    "                color = source2.find('div', class_ = 'color clearfix').a.find('div', class_ = 'arrow').p.string\n",
    "            except Exception as e:\n",
    "                color = None\n",
    "            try:\n",
    "                price = source2.find('p', class_ = 'price').em.children\n",
    "                sale_price = next(price).replace('￥','')\n",
    "                origin_price = next(price).string.replace('￥','')\n",
    "            except Exception as e:\n",
    "                sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "                origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "            try:\n",
    "                design = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[0].p.string\n",
    "            except Exception as e:\n",
    "                design = None\n",
    "            try:\n",
    "                material = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[1].p.string.replace('面料:','')\n",
    "            except Exception as e:\n",
    "                material = None\n",
    "            try:\n",
    "                stereotype = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[0].p.string\n",
    "            except Exception as e:\n",
    "                stereotype = None\n",
    "            try:\n",
    "                length = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[1].p.string\n",
    "            except Exception as e:\n",
    "                length = None\n",
    "            ## 把结果写入到 csv 文件中\n",
    "            csv_writer.writerow([category, goods_id, goods_name, color, sale_price, \n",
    "                                 origin_price, design, material, stereotype, length])\n",
    "            \n",
    "            ## 由于图片比较耗资源，暂时不爬取图片\n",
    "            '''\n",
    "            ## 爬取主图片\n",
    "            main_src = source2.find('ul', class_ = 'lineone').find('div', class_ = 'big_img').find('img').get('data-src')\n",
    "            img = requests.get(main_src, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            ## 爬取展示图片\n",
    "            show_src1 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[0].find('img').get('data-src')\n",
    "            img = requests.get(show_src1, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show1' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            show_src2 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[1].find('img').get('data-src')\n",
    "            img = requests.get(show_src2, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show2' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            show_src3 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[2].find('img').get('data-src')\n",
    "            img = requests.get(show_src3, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show3' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            show_src4 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[3].find('img').get('data-src')\n",
    "            img = requests.get(show_src4, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show4' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            ## 爬取细节图片\n",
    "            detail_src1 = source2.find('ul', class_ = 'linethree clearfix').find_all('li')[0].find('img').get('data-src')\n",
    "            img = requests.get(detail_src1, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-detail1' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            detail_src2 = source2.find('ul', class_ = 'linethree clearfix').find_all('li')[1].find('img').get('data-src')\n",
    "            img = requests.get(detail_src2, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-detail2' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            detail_src3 = source2.find('ul', class_ = 'linethree clearfix').find_all('li')[2].find('img').get('data-src')\n",
    "            img = requests.get(detail_src3, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-detail3' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            '''\n",
    "        # 显示进度\n",
    "        print(category, '第', page_num+1, '页')\n",
    "        print('------------')\n",
    "        # 每爬取一页，休息一下\n",
    "        time.sleep(30)\n",
    "# 没爬取一个品类，休息一下\n",
    "time.sleep(300)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毛织 第 1 页\n",
      "------------\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='www.ochirly.com.cn', port=80): Max retries exceeded with url: /p/1GZ3031740950.shtml (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10b808400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 150\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x10b808400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 639\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='www.ochirly.com.cn', port=80): Max retries exceeded with url: /p/1GZ3031740950.shtml (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10b808400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a8f58441a714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgoods_href\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoods_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0murl2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgoods_href\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msource2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gb18030'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgoods_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'编号 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='www.ochirly.com.cn', port=80): Max retries exceeded with url: /p/1GZ3031740950.shtml (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10b808400>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 连续爬取多个品类中途容易中断，因此单独爬取一个品类 =======================\n",
    "# ==================================================================\n",
    "## 选择需要爬取的品类\n",
    "ochirly_yifu = ochirly_category.loc[ochirly_category.cat2 == '毛织',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_yifu = ochirly_yifu.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，写入商品信息\n",
    "csv_file = open('./ochirly_goods_info.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['category', 'goods_id', 'goods_name', 'color', ' sale_price', \n",
    "                     'origin_price', 'design', 'material', 'stereotype', 'length'])\n",
    "## 获取商品信息\n",
    "category = ochirly_yifu.loc[0, 'cat2']\n",
    "for page_num in range(ochirly_yifu.loc[0, 'page_num']):\n",
    "    url1 = base_url + ochirly_yifu.loc[0, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "    page1 = requests.get(url1, headers = header)\n",
    "    source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "    goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "    for goods_info in goods_infos:\n",
    "        goods_href = goods_info.find('a').get('href')\n",
    "        url2 = base_url + goods_href\n",
    "        page2 = requests.get(url2, headers = header)\n",
    "        source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "        try:\n",
    "            goods_name = source2.find('p', class_ = 'title').string.replace('\\n', '').replace('\t', '')\n",
    "        except Exception as e:\n",
    "            goods_name = None\n",
    "        try:\n",
    "            color = source2.find('div', class_ = 'color clearfix').a.find('div', class_ = 'arrow').p.string\n",
    "        except Exception as e:\n",
    "            color = None\n",
    "        try:\n",
    "            price = source2.find('p', class_ = 'price').em.children\n",
    "            sale_price = next(price).replace('￥','')\n",
    "            origin_price = next(price).string.replace('￥','')\n",
    "        except Exception as e:\n",
    "            sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "            origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "        try:\n",
    "            design = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[0].p.string\n",
    "        except Exception as e:\n",
    "            design = None\n",
    "        try:\n",
    "            material = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[1].p.string.replace('面料:','')\n",
    "        except Exception as e:\n",
    "            material = None\n",
    "        try:\n",
    "            stereotype = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[0].p.string\n",
    "        except Exception as e:\n",
    "            stereotype = None\n",
    "        try:\n",
    "            length = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[1].p.string\n",
    "        except Exception as e:\n",
    "            length = None\n",
    "        ## 把结果写入到 csv 文件中\n",
    "        csv_writer.writerow([category, goods_id, goods_name, color, sale_price, \n",
    "                             origin_price, design, material, stereotype, length])\n",
    "\n",
    "    # 显示进度\n",
    "    print(category, '第', page_num+1, '页')\n",
    "    print('------------')\n",
    "    # 休息一下\n",
    "    time.sleep(30)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "￥489 \n",
      "￥969\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "url2 = 'http://www.ochirly.com.cn/p/1JY4081920090.shtml'\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "price = source2.find('p', class_ = 'price').em.children\n",
    "sale_p = next(price)\n",
    "origin_p = next(price).string\n",
    "print(sale_p)\n",
    "print(origin_p)\n",
    "print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489 \n",
      "969\n"
     ]
    }
   ],
   "source": [
    "url2 = 'http://www.ochirly.com.cn/p/1JY4081920090.shtml'\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "try:\n",
    "    goods_name = source2.find('p', class_ = 'title').string.replace('\\n', '').replace('\t', '')\n",
    "except Exception as e:\n",
    "    goods_name = None\n",
    "try:\n",
    "    color = source2.find('div', class_ = 'color clearfix').a.find('div', class_ = 'arrow').p.string\n",
    "except Exception as e:\n",
    "    color = None\n",
    "try:\n",
    "    price = source2.find('p', class_ = 'price').em.children\n",
    "    sale_price = next(price).replace('￥','')\n",
    "    origin_price = next(price).string.replace('￥','')\n",
    "except Exception as e:\n",
    "    sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "    origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "try:\n",
    "    design = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[0].p.string\n",
    "except Exception as e:\n",
    "    design = None\n",
    "try:\n",
    "    material = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[1].p.string.replace('面料:','')\n",
    "except Exception as e:\n",
    "    material = None\n",
    "try:\n",
    "    stereotype = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[0].p.string\n",
    "except Exception as e:\n",
    "    stereotype = None\n",
    "try:\n",
    "    length = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[1].p.string\n",
    "except Exception as e:\n",
    "    length = None\n",
    "\n",
    "print(sale_price)\n",
    "print(origin_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul class=\"lineone\">\n",
      "<li>\n",
      "<div class=\"zoom_wrap\" id=\"zoomWrap\">\n",
      "<div class=\"middle_img\" id=\"middleImg\">\n",
      "<img alt=\"翻领长款羽绒服 \" data-src=\"http://img1.ochirly.com.cn/wcsstore/TrendyCatalogAssetStore/images/trendy/ochirly/2018/d/1GY4330780532/1GY4330780532_m_8.jpg\" src=\"//img2.ochirly.com.cn/rs/common/images/blank.gif\"/>\n",
      "</div>\n",
      "<div class=\"big_img\" id=\"bigImg\">\n",
      "<img alt=\"翻领长款羽绒服 \" data-src=\"http://img1.ochirly.com.cn/wcsstore/TrendyCatalogAssetStore/images/trendy/ochirly/2018/d/1GY4330780532/1GY4330780532_b_8.jpg\" src=\"//img2.ochirly.com.cn/rs/common/images/blank.gif\"/>\n",
      "</div>\n",
      "</div>\n",
      "</li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "url1 = 'http://www.ochirly.com.cn/p/1GY4330780532.shtml'\n",
    "page1 = requests.get(url1, headers = header)\n",
    "source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "img_src = source1.find('ul', class_ = 'lineone')\n",
    "print(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9月5日到店\n"
     ]
    }
   ],
   "source": [
    "print(ochirly_newin.loc[7,'cat2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ochirly_newin = ochirly_category.loc[ochirly_category.cat1 == '新品',]\n",
    "print(ochirly_newin.shape)\n",
    "## 获取 goods_id 和场景名称\n",
    "for row_num in range(ochirly_newin.shape[0]):\n",
    "    #newin_date = ochirly_newin.loc[row_num, 'cat2']\n",
    "    print(row_num)\n",
    "    #print(newin_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/p/1GY1084160090.shtml\n",
      "1GH2085730510\n",
      "\r",
      "\t\t\t\t\t\t\t￥999\r",
      "\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "url2 = 'http://www.ochirly.com.cn/p/1GH2085730510.shtml'\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "print(goods_href)\n",
    "print(goods_id)\n",
    "print(goods_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "休闲\n",
      "1\n",
      "工作\n",
      "2\n",
      "逛街约会\n",
      "3\n",
      "度假\n",
      "4\n",
      "派对\n",
      "5\n",
      "SNOOPY限量版\n"
     ]
    }
   ],
   "source": [
    "ochirly_changjing = ochirly_category.loc[ochirly_category.cat1 == '场景',]\n",
    "## 获取 goods_id 和场景名称\n",
    "for row_num in range(ochirly_changjing.shape[0]):\n",
    "    changjing = ochirly_changjing.loc[row_num, 'cat2']\n",
    "    print(row_num)\n",
    "    print(changjing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
