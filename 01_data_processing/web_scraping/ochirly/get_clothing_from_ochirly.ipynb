{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载包\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "## 增加重连次数\n",
    "requests.adapters.DEFAULT_RETRIES = 5\n",
    "base_url = 'http://www.ochirly.com.cn'\n",
    "url_mid = 'list-'\n",
    "url_end = '-40-listedDate%20desc-0-0-0-0-0.shtml'\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/6.1.2107.204 Safari/537.36', \n",
    "          'Connection':'close'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>url</th>\n",
       "      <th>page_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>新品</td>\n",
       "      <td>3月到店</td>\n",
       "      <td>/New_In/New_in_March/</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>新品</td>\n",
       "      <td>4月3日到店</td>\n",
       "      <td>/New_In/New_in_4_3/</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>新品</td>\n",
       "      <td>4月10日到店</td>\n",
       "      <td>/New_In/New_in_4_10/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>新品</td>\n",
       "      <td>4月17日到店</td>\n",
       "      <td>/New_In/New_in_4_17/</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>新品</td>\n",
       "      <td>4月24日到店</td>\n",
       "      <td>/New_In/New_in_4_24/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>服装</td>\n",
       "      <td>连衣裙</td>\n",
       "      <td>/Clothing/Dresses/</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>服装</td>\n",
       "      <td>毛织</td>\n",
       "      <td>/Clothing/Sweaters/</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>服装</td>\n",
       "      <td>T恤</td>\n",
       "      <td>/Clothing/T_Shirts/</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>服装</td>\n",
       "      <td>衬衫</td>\n",
       "      <td>/Clothing/Shirts/</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>服装</td>\n",
       "      <td>半截裙</td>\n",
       "      <td>/Clothing/Skirts/</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>服装</td>\n",
       "      <td>裤装</td>\n",
       "      <td>/Clothing/Pants/</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>服装</td>\n",
       "      <td>外套</td>\n",
       "      <td>/Clothing/Jakets/</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1     cat2                    url  page_num\n",
       "0    新品     3月到店  /New_In/New_in_March/         5\n",
       "1    新品   4月3日到店    /New_In/New_in_4_3/         4\n",
       "2    新品  4月10日到店   /New_In/New_in_4_10/         1\n",
       "3    新品  4月17日到店   /New_In/New_in_4_17/         5\n",
       "4    新品  4月24日到店   /New_In/New_in_4_24/         1\n",
       "5    服装      连衣裙     /Clothing/Dresses/        11\n",
       "6    服装       毛织    /Clothing/Sweaters/        18\n",
       "7    服装       T恤    /Clothing/T_Shirts/        12\n",
       "8    服装       衬衫      /Clothing/Shirts/         9\n",
       "9    服装      半截裙      /Clothing/Skirts/        10\n",
       "10   服装       裤装       /Clothing/Pants/        11\n",
       "11   服装       外套      /Clothing/Jakets/         3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取商品分类信息:ochirly_category.csv\n",
    "ochirly_category = pd.read_table('./ochirly_category.csv', \n",
    "                                 sep = ',', \n",
    "                                 encoding = 'gbk')\n",
    "ochirly_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取新品的上市日期\n",
      "开始时间： 16:33:58\n",
      "新品上市日期爬取完成！\n",
      "完成时间： 16:36:35\n",
      "用时： 157 秒\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# 爬取新品的上市日期\n",
    "\n",
    "print('开始爬取新品的上市日期')\n",
    "xp_start_time = datetime.datetime.fromtimestamp(time.time())\n",
    "xp_start_time2 = datetime.datetime.strftime(xp_start_time, '%H:%M:%S')\n",
    "print('开始时间： ' + xp_start_time2)\n",
    "\n",
    "## 从ochirly_category表中筛选新品\n",
    "ochirly_newin = ochirly_category.loc[ochirly_category.cat1 == '新品',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_newin = ochirly_newin.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，把goods_id 和上市日期写入\n",
    "csv_file = open('./ochirly_goods_newin.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['goods_id', 'newin_date'])\n",
    "## 获取 goods_id 和上市日期\n",
    "for row_num in range(ochirly_newin.shape[0]):\n",
    "    newin_date = ochirly_newin.loc[row_num, 'cat2'].replace('到店', '').replace('月', '-').replace('日','')\n",
    "    for page_num in range(ochirly_newin.loc[row_num, 'page_num']):\n",
    "        url1 = base_url + ochirly_newin.loc[row_num, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "        page1 = requests.get(url1, headers = header)\n",
    "        source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_list = source1.find('div', class_ = 'product_list')\n",
    "        ## 为避免出现NoneType对象没有属性find_all，用isinstance()函数将空类型过滤掉\n",
    "        if isinstance(goods_list, bs4.element.Tag):\n",
    "            goods_infos = goods_list.find_all('li')\n",
    "            for goods_info in goods_infos:\n",
    "                goods_href = goods_info.find('a').get('href')\n",
    "                url2 = base_url + goods_href\n",
    "                page2 = requests.get(url2, headers = header)\n",
    "                source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "                goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "                ## 把结果写入到 csv 文件中\n",
    "                csv_writer.writerow([goods_id, newin_date])\n",
    "            # 休息一下\n",
    "            time.sleep(3)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()\n",
    "\n",
    "print('新品上市日期爬取完成！')\n",
    "xp_end_time = datetime.datetime.fromtimestamp(time.time())\n",
    "xp_end_time2 = datetime.datetime.strftime(xp_end_time, '%H:%M:%S')\n",
    "print('完成时间： ' + xp_end_time2)\n",
    "xp_use_time = xp_end_time - xp_start_time\n",
    "print('用时： ' + str(xp_use_time.seconds) + ' 秒')\n",
    "print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取服装\n",
      "开始时间： 16:43:10\n",
      "连衣裙 第 1 页\n",
      "------------\n",
      "连衣裙 第 2 页\n",
      "------------\n",
      "连衣裙 第 3 页\n",
      "------------\n",
      "连衣裙 第 4 页\n",
      "------------\n",
      "连衣裙 第 5 页\n",
      "------------\n",
      "连衣裙 第 6 页\n",
      "------------\n",
      "连衣裙 第 7 页\n",
      "------------\n",
      "连衣裙 第 8 页\n",
      "------------\n",
      "连衣裙 第 9 页\n",
      "------------\n",
      "连衣裙 第 10 页\n",
      "------------\n",
      "连衣裙 第 11 页\n",
      "------------\n",
      "毛织 第 1 页\n",
      "------------\n",
      "毛织 第 2 页\n",
      "------------\n",
      "毛织 第 3 页\n",
      "------------\n",
      "毛织 第 4 页\n",
      "------------\n",
      "毛织 第 5 页\n",
      "------------\n",
      "毛织 第 6 页\n",
      "------------\n",
      "毛织 第 7 页\n",
      "------------\n",
      "毛织 第 8 页\n",
      "------------\n",
      "毛织 第 9 页\n",
      "------------\n",
      "毛织 第 10 页\n",
      "------------\n",
      "毛织 第 11 页\n",
      "------------\n",
      "毛织 第 12 页\n",
      "------------\n",
      "毛织 第 13 页\n",
      "------------\n",
      "毛织 第 14 页\n",
      "------------\n",
      "毛织 第 15 页\n",
      "------------\n",
      "毛织 第 16 页\n",
      "------------\n",
      "毛织 第 17 页\n",
      "------------\n",
      "毛织 第 18 页\n",
      "------------\n",
      "T恤 第 1 页\n",
      "------------\n",
      "T恤 第 2 页\n",
      "------------\n",
      "T恤 第 3 页\n",
      "------------\n",
      "T恤 第 4 页\n",
      "------------\n",
      "T恤 第 5 页\n",
      "------------\n",
      "T恤 第 6 页\n",
      "------------\n",
      "T恤 第 7 页\n",
      "------------\n",
      "T恤 第 8 页\n",
      "------------\n",
      "T恤 第 9 页\n",
      "------------\n",
      "T恤 第 10 页\n",
      "------------\n",
      "T恤 第 11 页\n",
      "------------\n",
      "T恤 第 12 页\n",
      "------------\n",
      "衬衫 第 1 页\n",
      "------------\n",
      "衬衫 第 2 页\n",
      "------------\n",
      "衬衫 第 3 页\n",
      "------------\n",
      "衬衫 第 4 页\n",
      "------------\n",
      "衬衫 第 5 页\n",
      "------------\n",
      "衬衫 第 6 页\n",
      "------------\n",
      "衬衫 第 7 页\n",
      "------------\n",
      "衬衫 第 8 页\n",
      "------------\n",
      "衬衫 第 9 页\n",
      "------------\n",
      "半截裙 第 1 页\n",
      "------------\n",
      "半截裙 第 2 页\n",
      "------------\n",
      "半截裙 第 3 页\n",
      "------------\n",
      "半截裙 第 4 页\n",
      "------------\n",
      "半截裙 第 5 页\n",
      "------------\n",
      "半截裙 第 6 页\n",
      "------------\n",
      "半截裙 第 7 页\n",
      "------------\n",
      "半截裙 第 8 页\n",
      "------------\n",
      "半截裙 第 9 页\n",
      "------------\n",
      "半截裙 第 10 页\n",
      "------------\n",
      "裤装 第 1 页\n",
      "------------\n",
      "裤装 第 2 页\n",
      "------------\n",
      "裤装 第 3 页\n",
      "------------\n",
      "裤装 第 4 页\n",
      "------------\n",
      "裤装 第 5 页\n",
      "------------\n",
      "裤装 第 6 页\n",
      "------------\n",
      "裤装 第 7 页\n",
      "------------\n",
      "裤装 第 8 页\n",
      "------------\n",
      "裤装 第 9 页\n",
      "------------\n",
      "裤装 第 10 页\n",
      "------------\n",
      "裤装 第 11 页\n",
      "------------\n",
      "外套 第 1 页\n",
      "------------\n",
      "外套 第 2 页\n",
      "------------\n",
      "外套 第 3 页\n",
      "------------\n",
      "服装爬取完成！\n",
      "完成时间： 17:37:55\n",
      "用时： 3284 秒\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 爬取所有服装商品\n",
    "# 多个大类一起爬取\n",
    "# ==================================================================\n",
    "\n",
    "print('开始爬取服装')\n",
    "fz_start_time = datetime.datetime.fromtimestamp(time.time())\n",
    "fz_start_time2 = datetime.datetime.strftime(fz_start_time, '%H:%M:%S')\n",
    "print('开始时间： ' + fz_start_time2)\n",
    "\n",
    "## 从ochirly_category表中筛选服装的信息\n",
    "ochirly_yifu = ochirly_category.loc[ochirly_category.cat1 == '服装',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_yifu = ochirly_yifu.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，写入商品信息\n",
    "csv_file = open('./ochirly_goods_info.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['category','goods_id','goods_name','color','sale_price','origin_price',\n",
    "                     'design', 'material', 'stereotype', 'length', 'page_num'])\n",
    "## 获取商品信息\n",
    "for row_num in range(ochirly_yifu.shape[0]):\n",
    "    category = ochirly_yifu.loc[row_num, 'cat2']\n",
    "    for page_num in range(ochirly_yifu.loc[row_num, 'page_num']):\n",
    "        url1 = base_url + ochirly_yifu.loc[row_num, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "        page1 = requests.get(url1, headers = header)\n",
    "        source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "        for goods_info in goods_infos:\n",
    "            goods_href = goods_info.find('a').get('href')\n",
    "            url2 = base_url + goods_href\n",
    "            page2 = requests.get(url2, headers = header)\n",
    "            source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "            goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "            try:\n",
    "                goods_name = source2.find('p', class_ = 'title').string.replace('\\n', '').replace('\t', '')\n",
    "            except Exception as e:\n",
    "                goods_name = None\n",
    "            try:\n",
    "                color = source2.find('div', class_ = 'color clearfix').a.find('div', class_ = 'arrow').p.string\n",
    "            except Exception as e:\n",
    "                color = None\n",
    "            try:\n",
    "                price = source2.find('p', class_ = 'price').em.children\n",
    "                sale_price = next(price).replace('￥','')\n",
    "                origin_price = next(price).string.replace('￥','')\n",
    "            except Exception as e:\n",
    "                sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "                origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "            try:\n",
    "                design = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[0].p.string\n",
    "            except Exception as e:\n",
    "                design = None\n",
    "            try:\n",
    "                material = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[1].p.string.replace('面料:','')\n",
    "            except Exception as e:\n",
    "                material = None\n",
    "            try:\n",
    "                stereotype = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[0].p.string\n",
    "            except Exception as e:\n",
    "                stereotype = None\n",
    "            try:\n",
    "                length = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[1].p.string\n",
    "            except Exception as e:\n",
    "                length = None\n",
    "            ## 把结果写入到 csv 文件中\n",
    "            csv_writer.writerow([category,goods_id,goods_name,color,sale_price,origin_price,\n",
    "                                 design,material,stereotype,length,page_num+1])\n",
    "            \n",
    "            ## 由于图片比较耗资源，暂时不爬取图片\n",
    "            '''\n",
    "            ## 爬取主图片\n",
    "            main_src = source2.find('ul', class_ = 'lineone').find('div', class_ = 'big_img').find('img').get('data-src')\n",
    "            img = requests.get(main_src, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            ## 爬取展示图片\n",
    "            show_src1 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[0].find('img').get('data-src')\n",
    "            img = requests.get(show_src1, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show1' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            show_src2 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[1].find('img').get('data-src')\n",
    "            img = requests.get(show_src2, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show2' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            show_src3 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[2].find('img').get('data-src')\n",
    "            img = requests.get(show_src3, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show3' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            show_src4 = source2.find('ul', class_ = 'linetwo clearfix').find_all('li')[3].find('img').get('data-src')\n",
    "            img = requests.get(show_src4, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-show4' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            ## 爬取细节图片\n",
    "            detail_src1 = source2.find('ul', class_ = 'linethree clearfix').find_all('li')[0].find('img').get('data-src')\n",
    "            img = requests.get(detail_src1, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-detail1' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            detail_src2 = source2.find('ul', class_ = 'linethree clearfix').find_all('li')[1].find('img').get('data-src')\n",
    "            img = requests.get(detail_src2, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-detail2' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            detail_src3 = source2.find('ul', class_ = 'linethree clearfix').find_all('li')[2].find('img').get('data-src')\n",
    "            img = requests.get(detail_src3, timeout = 3)\n",
    "            img_name = 'ochirly_goods_image/' + goods_id + '-detail3' + '.jpg'\n",
    "            fp = open(img_name, 'wb')\n",
    "            fp.write(img.content)\n",
    "            fp.close()\n",
    "            time.sleep(2)\n",
    "            '''\n",
    "        # 显示进度\n",
    "        print(category, '第', page_num+1, '页')\n",
    "        print('------------')\n",
    "        # 每爬取一页，休息一下\n",
    "        time.sleep(30)\n",
    "# 每爬取一个品类，休息一下\n",
    "time.sleep(300)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()\n",
    "\n",
    "print('服装爬取完成！')\n",
    "fz_end_time = datetime.datetime.fromtimestamp(time.time())\n",
    "fz_end_time2 = datetime.datetime.strftime(fz_end_time, '%H:%M:%S')\n",
    "print('完成时间： ' + fz_end_time2)\n",
    "fz_use_time = fz_end_time - fz_start_time\n",
    "print('用时： ' + str(fz_use_time.seconds) + ' 秒')\n",
    "print('=========================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取商品的场景分类\n",
      "开始时间： 14:14:34\n",
      "场景分类爬取完成！\n",
      "完成时间： 14:15:15\n",
      "用时： 40 秒\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# 爬取商品的场景分类\n",
    "\n",
    "print('开始爬取商品的场景分类')\n",
    "cj_start_time = datetime.datetime.fromtimestamp(time.time())\n",
    "cj_start_time2 = datetime.datetime.strftime(cj_start_time, '%H:%M:%S')\n",
    "print('开始时间： ' + cj_start_time2)\n",
    "\n",
    "## 从ochirly_category表中筛选场景购的信息\n",
    "ochirly_changjing = ochirly_category.loc[ochirly_category.cat1 == '场景',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_changjing = ochirly_changjing.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，把goods_id 和场景名称写入\n",
    "csv_file = open('./ochirly_goods_changjing.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['goods_id', 'changjing'])\n",
    "## 获取 goods_id 和场景名称\n",
    "for row_num in range(ochirly_changjing.shape[0]):\n",
    "    changjing = ochirly_changjing.loc[row_num, 'cat2']\n",
    "    for page_num in range(ochirly_changjing.loc[row_num, 'page_num']):\n",
    "        url1 = base_url + ochirly_changjing.loc[row_num, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "        page1 = requests.get(url1, headers = header)\n",
    "        source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "        for goods_info in goods_infos:\n",
    "            goods_href = goods_info.find('a').get('href')\n",
    "            url2 = base_url + goods_href\n",
    "            page2 = requests.get(url2, headers = header)\n",
    "            source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "            goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "            ## 把结果写入到 csv 文件中\n",
    "            csv_writer.writerow([goods_id, changjing])\n",
    "        # 休息一下\n",
    "        time.sleep(3)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()\n",
    "\n",
    "print('场景分类爬取完成！')\n",
    "cj_end_time = datetime.datetime.fromtimestamp(time.time())\n",
    "cj_end_time2 = datetime.datetime.strftime(cj_end_time, '%H:%M:%S')\n",
    "print('完成时间： ' + cj_end_time2)\n",
    "cj_use_time = cj_end_time - cj_start_time\n",
    "print('用时： ' + str(cj_use_time.seconds) + ' 秒')\n",
    "print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取\n",
      "开始时间： 11:44:45\n",
      "连衣裙 第 1 页\n",
      "------------\n",
      "连衣裙 第 2 页\n",
      "------------\n",
      "连衣裙 第 3 页\n",
      "------------\n",
      "爬取完成！\n",
      "完成时间： 11:46:10\n",
      "用时： 84 秒\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# 连续爬取多个品类中途容易中断，因此单独爬取一个品类 =======================\n",
    "# ==================================================================\n",
    "\n",
    "print('开始爬取')\n",
    "fz_start_time = datetime.datetime.fromtimestamp(time.time())\n",
    "fz_start_time2 = datetime.datetime.strftime(fz_start_time, '%H:%M:%S')\n",
    "print('开始时间： ' + fz_start_time2)\n",
    "\n",
    "## 选择需要爬取的品类\n",
    "ochirly_yifu = ochirly_category.loc[ochirly_category.cat2 == '连衣裙',]\n",
    "## 重置row index为从0开始\n",
    "ochirly_yifu = ochirly_yifu.reset_index(drop = True)\n",
    "## 打开一个 csv 文件，写入商品信息\n",
    "csv_file = open('./ochirly_goods_info.csv', \n",
    "                'w', \n",
    "                newline = '', \n",
    "                encoding = 'utf-8-sig')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['category','goods_id','goods_name','color','sale_price','origin_price',\n",
    "                     'design', 'material', 'stereotype', 'length', 'page_num'])\n",
    "## 获取商品信息\n",
    "category = ochirly_yifu.loc[0, 'cat2']\n",
    "for page_num in range(ochirly_yifu.loc[0, 'page_num']):\n",
    "    s1 = requests.session()\n",
    "    s1.keep_alive = False\n",
    "    url1 = base_url + ochirly_yifu.loc[0, 'url'] + url_mid + str(page_num+1) + url_end\n",
    "    page1 = s1.get(url1, headers = header)\n",
    "    source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "    goods_infos = source1.find('div', class_ = 'product_list').find_all('li')\n",
    "    for goods_info in goods_infos:\n",
    "        goods_href = goods_info.find('a').get('href')\n",
    "        s2 = requests.session()\n",
    "        s2.keep_alive = False\n",
    "        url2 = base_url + goods_href\n",
    "        page2 = s2.get(url2, headers = header)\n",
    "        source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "        goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "        try:\n",
    "            goods_name = source2.find('p', class_ = 'title').string.replace('\\n', '').replace('\t', '')\n",
    "        except Exception as e:\n",
    "            goods_name = None\n",
    "        try:\n",
    "            color = source2.find('div', class_ = 'color clearfix').find('a', class_ = 'item active').find('div', class_ = 'arrow').p.string\n",
    "        except Exception as e:\n",
    "            color = None\n",
    "        try:\n",
    "            price = source2.find('p', class_ = 'price').em.children\n",
    "            sale_price = next(price).replace('￥','')\n",
    "            origin_price = next(price).string.replace('￥','')\n",
    "        except Exception as e:\n",
    "            sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "            origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "        try:\n",
    "            design = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[0].p.string\n",
    "        except Exception as e:\n",
    "            design = None\n",
    "        try:\n",
    "            material = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[1].p.string.replace('面料:','')\n",
    "        except Exception as e:\n",
    "            material = None\n",
    "        try:\n",
    "            stereotype = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[0].p.string\n",
    "        except Exception as e:\n",
    "            stereotype = None\n",
    "        try:\n",
    "            length = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[1].p.string\n",
    "        except Exception as e:\n",
    "            length = None\n",
    "        ## 把结果写入到 csv 文件中\n",
    "        csv_writer.writerow([category,goods_id,goods_name,color,sale_price,origin_price,\n",
    "                             design,material,stereotype,length,page_num+1])\n",
    "\n",
    "    # 显示进度\n",
    "    print(category, '第', page_num+1, '页')\n",
    "    print('------------')\n",
    "    # 休息一下\n",
    "    time.sleep(10)\n",
    "# 关闭 csv_file\n",
    "csv_file.close()\n",
    "\n",
    "print('爬取完成！')\n",
    "fz_end_time = datetime.datetime.fromtimestamp(time.time())\n",
    "fz_end_time2 = datetime.datetime.strftime(fz_end_time, '%H:%M:%S')\n",
    "print('完成时间： ' + fz_end_time2)\n",
    "fz_use_time = fz_end_time - fz_start_time\n",
    "print('用时： ' + str(fz_use_time.seconds) + ' 秒')\n",
    "print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goods_id: 1GZ3089890090\n",
      "<p class=\"price\" data-group-id=\"895#\" data-group-price=\"895:1:399.0:CNY:OCH-官网-在售折扣品(大于5折款)#\" data-list-price=\"669\" data-offer-price=\"669\" data-sku=\"1GZ3089890090\" id=\"priceOfSku1GZ3089890090\">\r\n",
      "\t\t\t\t\t\t\t￥669\r\n",
      "\t\t\t\t\t\t</p>\n"
     ]
    }
   ],
   "source": [
    "## 关于商品的售价和吊牌价\n",
    "\n",
    "url2 = 'http://www.ochirly.com.cn/p/1GZ3089890090.shtml'\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "price = source2.find('p', class_ = 'price')\n",
    "#sale_price = next(price).replace('￥','')\n",
    "#origin_price = next(price).string.replace('￥','')\n",
    "\n",
    "'''\n",
    "try:\n",
    "    price = source2.find('p', class_ = 'price').em.children\n",
    "    sale_price = next(price).replace('￥','')\n",
    "    origin_price = next(price).string.replace('￥','')\n",
    "except Exception as e:\n",
    "    sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "    origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "\n",
    "'''\n",
    "\n",
    "print('goods_id:', goods_id)\n",
    "#print('sale_price:' + sale_price)\n",
    "#print('origin_price' + origin_price)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goods_id: 1JY4330590030\n",
      "goods_name: \r",
      "可拆帽印花羽绒服\r\n",
      "color: 灰色\n",
      "sale_price: 1090 \n",
      "origin_price: 1890\n"
     ]
    }
   ],
   "source": [
    "url2 = 'http://www.ochirly.com.cn/p/1JY4330590030.shtml'\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "try:\n",
    "    goods_name = source2.find('p', class_ = 'title').string.replace('\\n', '').replace('\t', '')\n",
    "except Exception as e:\n",
    "    goods_name = None\n",
    "try:\n",
    "    color = source2.find('div', class_ = 'color clearfix').find('a', class_ = 'item active').find('div', class_ = 'arrow').p.string\n",
    "except Exception as e:\n",
    "    color = None\n",
    "try:\n",
    "    price = source2.find('p', class_ = 'price').em.children\n",
    "    sale_price = next(price).replace('￥','')\n",
    "    origin_price = next(price).string.replace('￥','')\n",
    "except Exception as e:\n",
    "    sale_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "    origin_price = source2.find('p', class_ = 'price').string.replace('\\n', '').replace('\t', '').replace('￥','')\n",
    "try:\n",
    "    design = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[0].p.string\n",
    "except Exception as e:\n",
    "    design = None\n",
    "try:\n",
    "    material = source2.find(attrs = {'id':'detailContent1'}).find_all('li')[1].p.string.replace('面料:','')\n",
    "except Exception as e:\n",
    "    material = None\n",
    "try:\n",
    "    stereotype = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[0].p.string\n",
    "except Exception as e:\n",
    "    stereotype = None\n",
    "try:\n",
    "    length = source2.find(attrs = {'id':'detailContent2'}).find_all('li')[1].p.string\n",
    "except Exception as e:\n",
    "    length = None\n",
    "print('goods_id:', goods_id)\n",
    "print('goods_name:', goods_name)\n",
    "print('color:', color)\n",
    "print('sale_price:', sale_price)\n",
    "print('origin_price:', origin_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ZZ1018880530\n"
     ]
    }
   ],
   "source": [
    "## 单独查询一个商品\n",
    "url = 'http://www.ochirly.com.cn/New_In/New_in_1_23/list.shtml'\n",
    "page1 = requests.get(url, headers = header)\n",
    "source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_info = source1.find('div', class_ = 'product_list').find('li')\n",
    "goods_href = goods_info.find('a').get('href')\n",
    "url2 = base_url + goods_href\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "print(goods_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor goods_info in goods_infos:\\n    goods_href = goods_info.find('a').get('href')\\n    url2 = base_url + goods_href\\n    page2 = requests.get(url2, headers = header)\\n    source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\\n    goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\\n    print(goods_id)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 查询多个商品\n",
    "url1 = 'http://www.ochirly.com.cn/p/1GY4330780532.shtml'\n",
    "page1 = requests.get(url1, headers = header)\n",
    "source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_infos = source1.find('div', class_ = 'product_list')##.find_all('li')\n",
    "print(goods_infos)\n",
    "print(type(goods_info))\n",
    "'''\n",
    "for goods_info in goods_infos:\n",
    "    goods_href = goods_info.find('a').get('href')\n",
    "    url2 = base_url + goods_href\n",
    "    page2 = requests.get(url2, headers = header)\n",
    "    source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "    goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "    print(goods_id)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul class=\"lineone\">\n",
      "<li>\n",
      "<div class=\"zoom_wrap\" id=\"zoomWrap\">\n",
      "<div class=\"middle_img\" id=\"middleImg\">\n",
      "<img alt=\"翻领长款羽绒服 \" data-src=\"http://img1.ochirly.com.cn/wcsstore/TrendyCatalogAssetStore/images/trendy/ochirly/2018/d/1GY4330780532/1GY4330780532_m_8.jpg\" src=\"//img2.ochirly.com.cn/rs/common/images/blank.gif\"/>\n",
      "</div>\n",
      "<div class=\"big_img\" id=\"bigImg\">\n",
      "<img alt=\"翻领长款羽绒服 \" data-src=\"http://img1.ochirly.com.cn/wcsstore/TrendyCatalogAssetStore/images/trendy/ochirly/2018/d/1GY4330780532/1GY4330780532_b_8.jpg\" src=\"//img2.ochirly.com.cn/rs/common/images/blank.gif\"/>\n",
      "</div>\n",
      "</div>\n",
      "</li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "url1 = 'http://www.ochirly.com.cn/p/1GY4330780532.shtml'\n",
    "page1 = requests.get(url1, headers = header)\n",
    "source1 = BeautifulSoup(page1.content, 'html.parser', from_encoding = 'gb18030')\n",
    "img_src = source1.find('ul', class_ = 'lineone')\n",
    "print(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9月5日到店\n"
     ]
    }
   ],
   "source": [
    "print(ochirly_newin.loc[7,'cat2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ochirly_newin = ochirly_category.loc[ochirly_category.cat1 == '新品',]\n",
    "print(ochirly_newin.shape)\n",
    "## 获取 goods_id 和场景名称\n",
    "for row_num in range(ochirly_newin.shape[0]):\n",
    "    #newin_date = ochirly_newin.loc[row_num, 'cat2']\n",
    "    print(row_num)\n",
    "    #print(newin_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/p/1GY1084160090.shtml\n",
      "1GH2085730510\n",
      "\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "url2 = 'http://www.ochirly.com.cn/p/1GH2085730510.shtml'\n",
    "page2 = requests.get(url2, headers = header)\n",
    "source2 = BeautifulSoup(page2.content, 'html.parser', from_encoding = 'gb18030')\n",
    "goods_id = source2.find('p', class_ = 'number').string.replace('编号 ', '')\n",
    "print(goods_href)\n",
    "print(goods_id)\n",
    "print(goods_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "休闲\n",
      "1\n",
      "工作\n",
      "2\n",
      "逛街约会\n",
      "3\n",
      "度假\n",
      "4\n",
      "派对\n",
      "5\n",
      "SNOOPY限量版\n"
     ]
    }
   ],
   "source": [
    "ochirly_changjing = ochirly_category.loc[ochirly_category.cat1 == '场景',]\n",
    "## 获取 goods_id 和场景名称\n",
    "for row_num in range(ochirly_changjing.shape[0]):\n",
    "    changjing = ochirly_changjing.loc[row_num, 'cat2']\n",
    "    print(row_num)\n",
    "    print(changjing)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
