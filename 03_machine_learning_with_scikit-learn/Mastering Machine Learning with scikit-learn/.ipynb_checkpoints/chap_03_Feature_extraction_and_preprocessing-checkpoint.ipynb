{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 03 Feature Extraction And Preprocessing\n",
    "***\n",
    "\n",
    "- 3.1 Extracting features from categorical variables\n",
    "- 3.2 Extracting features from text\n",
    "- 2.3 polynomial regression 多项式回归\n",
    "- 2.4 how to train models & cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Extracting features from categorical variables\n",
    "***\n",
    "\n",
    "对分类变量一般使用one-hot编码，有人会想到用整数0,1,2,...来分别表示分类变量，但一般不建议这么做，因为编码成整数，实际上添加了一个人工信息，即这些变量是有序的，0<1<2<..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对分类变量一般使用one-hot编码\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "instances = [{'city':'New York'}, {'city':'San Francisco'}, {'city':'Chapel Hill'}]\n",
    "onehot_encoder.fit_transform(instances).toarray()\n",
    "# 说明：我们发现城市编码的顺序跟instances中城市的顺序不一样，这是因为编码按照了城市名称升序排列了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extracting features from text\n",
    "\n",
    "- bag of words:词袋\n",
    "\n",
    "词袋类似one-hot编码，可以认为是后者的一个特例。\n",
    "\n",
    "词袋模型忽略文本中词语的语法，词语出现的顺序等。\n",
    "\n",
    "词袋模型基于一个直觉假设：含有相似词语的文本，具有相似的含义。虽然这个直觉对于有些文本是不一定适用的，有时不同文本虽然含有相同词汇，但是顺序不同，含义就不同。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words:\n",
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "----------------------------------------\n",
      "Vocabulary\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "# bag of words\n",
    "\n",
    "## 导入类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## 语料库corpus\n",
    "corpus = ['UNC played Duke in basketball', \n",
    "          'Duke lost the basketball game']\n",
    "## 说明：\n",
    "## corpus语料中的所有单词组成了词典\n",
    "## 上面给出的语料包含8个不重复的单词\n",
    "\n",
    "## 得到词袋模型\n",
    "vectorizer = CountVectorizer()\n",
    "print('bag of words:')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "\n",
    "## 从语料库切词得到的字典\n",
    "print('----------------------------------------')\n",
    "print('Vocabulary')\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "## 解释\n",
    "## Vocabulary中的单词是有顺序的，第一个是basketball\n",
    "## 词袋模型的编码与单词的顺序是对应的，1表示文本中含有该单词，0表示不含有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words:\n",
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "-----------------------------------------\n",
      "Vocabulary\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本1和文本2的距离: [[ 2.44948974]]\n",
      "文本1和文本3的距离: [[ 2.64575131]]\n",
      "文本2和文本3的距离: [[ 2.64575131]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# bag of words(2)\n",
    "## 增加一条文本\n",
    "\n",
    "## 导入类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## 语料库\n",
    "corpus = ['UNC played Duke in basketball', \n",
    "          'Duke lost the basketball game', \n",
    "          'I ate a sandwich']\n",
    "\n",
    "## 词袋模型\n",
    "vectorizer = CountVectorizer()\n",
    "print('bag of words:')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "\n",
    "## 字典\n",
    "print('-----------------------------------------')\n",
    "print('Vocabulary')\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "## 计算三条文本之间的距离\n",
    "## 文本1和文本2较为相似，与文本3差异较大\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "counts = [[0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
    "          [0, 1, 1, 1, 0, 1, 0, 0, 1, 0],\n",
    "          [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
    "# 文本两两比较，计算距离\n",
    "## euclidean distance：sqrt(sum((xi - yi)^2))\n",
    "print('文本1和文本2的距离:', euclidean_distances(counts[0], counts[1]))\n",
    "print('文本1和文本3的距离:', euclidean_distances(counts[0], counts[2]))\n",
    "print('文本2和文本3的距离:', euclidean_distances(counts[1], counts[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### bag of words的问题\n",
    "\n",
    "**1.如果语料库巨大，那么词袋是一个非常大的稀疏矩阵，这样的稀疏矩阵对建模是非常不利的：**\n",
    "\n",
    "- 1.1.稀疏矩阵越大，那么就需要越多的训练样本，如果训练样本过小，容易造成过拟合；\n",
    "\n",
    "- 1.2.稀疏矩阵越大，需要的磁盘空间也越大；\n",
    "\n",
    "**2.减小稀疏矩阵的方法：**\n",
    "\n",
    "- 2.1.将文本全部转成小写，因为词袋模型已经忽略了单词的顺序和语法，因此大小写已经无意义了，全部转成小写，可以减小单词的数量；\n",
    "\n",
    "- 2.2.使用停止词stop words，主要包括以下几种类型：\n",
    "\n",
    "    - 限定词：如the, a, an;\n",
    "    - 助动词：如do, will, be;\n",
    "    - 介词：如on, around, beneath;\n",
    "    - 人工标注的停止词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag od words:\n",
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "----------------------------------------\n",
      "Vocabulary\n",
      "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n",
      "T1 VS T2: [[ 2.]]\n",
      "T1 VS T3: [[ 2.44948974]]\n",
      "T2 VS T3: [[ 2.44948974]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# stop words\n",
    "\n",
    "## 导入类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## 语料库\n",
    "corpus = ['UNC played Duke in basketball', \n",
    "          'Duke lost the basketball game', \n",
    "          'I ate a sandwich']\n",
    "\n",
    "## 切词，得到词袋模型，使用stop words\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "## 查看词袋模型结果和字典\n",
    "print('bag od words:')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Vocabulary')\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "### 从字典我们看到，没有了the和in两个单词\n",
    "\n",
    "## 计算距离\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "counts = [[0, 1, 1, 0, 0, 1, 0, 1],\n",
    "          [0, 1, 1, 1, 1, 0, 0, 0],\n",
    "          [1, 0, 0, 0, 0, 0, 1, 0]]\n",
    "print('T1 VS T2:', euclidean_distances(counts[0], counts[1]))\n",
    "print('T1 VS T3:', euclidean_distances(counts[0], counts[2]))\n",
    "print('T2 VS T3:', euclidean_distances(counts[1], counts[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### stemming and lemmatization\n",
    "\n",
    "一个英文单词有变形词和衍生词，如jumping，jumps的原形是jump。\n",
    "\n",
    "进行文本分析时，我们需要把变形词和衍生词还原为原形，以便缩小矩阵大小，也更有利于建模。\n",
    "\n",
    "- stemming:词干提取\n",
    "- lemmatization:词形还原\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words:\n",
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "------------------------------\n",
      "Vocabulary:\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "## 导入类\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## 语料库\n",
    "corpus = ['He ate the sandwiches', \n",
    "          'Every sandwich was eaten by him']\n",
    "\n",
    "## 切词并获得词袋模型，使用停止词\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print('bag of words:')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print('------------------------------')\n",
    "print('Vocabulary:')\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "## 说明：\n",
    "## 两条信息的意思其实是一样的，但是词袋模型却完全不一样，这是因为单词变化导致的，因此我们需要利用词形还原和词干提取的方法来消除单词变化的影响；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize gathering as noun: gathering\n",
      "lemmatize gathering as verb: gather\n",
      "Stem: gather\n"
     ]
    }
   ],
   "source": [
    "# use nltk to stem and lemmatize the corpus\n",
    "\n",
    "## 1.lemmatization\n",
    "\n",
    "## 语料库\n",
    "corpus = ['I am gathering ingredients for the sandwich.', \n",
    "          'There are many wizards at the gathering.']\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('lemmatize gathering as noun:', lemmatizer.lemmatize('gathering', 'n'))\n",
    "print('lemmatize gathering as verb:', lemmatizer.lemmatize('gathering', 'v'))\n",
    "## 说明：\n",
    "## lemmatization根据单词的词性，进行不同的处理\n",
    "\n",
    "## 2.stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print('Stem:', stemmer.stem('gathering'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n",
      "bag of words:\n",
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "----------------------------------------\n",
      "Vocabulary\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n",
      "T1 VS T2: [[ 2.]]\n",
      "bag of words:\n",
      "[[1 1]\n",
      " [1 1]]\n",
      "---------------------------------------------\n",
      "Vocabulary:\n",
      "{'eat': 0, 'sandwich': 1}\n",
      "T1 VS T2: [[ 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "d:\\application\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# lemmatize test corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = ['He ate the sandwiches', \n",
    "          'Every sandwich was eaten by him']\n",
    "## stemming\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])\n",
    "## lemmatization\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "print('Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus])\n",
    "\n",
    "## 词袋模型对比\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorize = CountVectorizer(binary = True, stop_words = 'english')\n",
    "\n",
    "### 原corpus\n",
    "print('bag of words:')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print('----------------------------------------')\n",
    "print('Vocabulary')\n",
    "print(vectorizer.vocabulary_)\n",
    "### 计算距离\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "counts = [[1, 0, 0, 1],\n",
    "          [0, 1, 1, 0]]\n",
    "print('T1 VS T2:', euclidean_distances(counts[0], counts[1]))\n",
    "\n",
    "### 新corpus\n",
    "lemm_corpus = ['He eat the sandwich', \n",
    "               'Every sandwich be eat by him']\n",
    "print('bag of words:')\n",
    "print(vectorizer.fit_transform(lemm_corpus).todense())\n",
    "print('---------------------------------------------')\n",
    "print('Vocabulary:')\n",
    "print(vectorizer.vocabulary_)\n",
    "counts = [[1, 1], \n",
    "          [1, 1]]\n",
    "print('T1 VS T2:', euclidean_distances(counts[0], counts[1]))\n",
    "\n",
    "# 疑问：怎么把词形还原的结果跟原始的文本对应，同时保存起来，并进行计算；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "**1.TF: Term Frequency，词频，指一个词在文档中出现的次数。当比较不同文档时，为了减小因为文档长短带来的误差，会对词频进行标准化，标准化的方法有：**\n",
    "\n",
    "- 1.1 TF = 某词出现的次数 / 该文档的总词数；\n",
    "- 1.2 TF = 某词出现的次数 / 文档中出现次数最多的那个词的出现次数；\n",
    "\n",
    "**2.IDF：Inverse Documents Frequency，逆文档频率，其假设是：越常见的词出现在我们研究的文档中，则其重要性越低，越不常见的词出现了，其重要性越高；其计算方式如下：**\n",
    "\n",
    "根据我们选择的语料库：\n",
    "\n",
    "- IDF = log(语料库的总文档数 / （包含某词的文档数 + 1）)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data standardization\n",
    "***\n",
    "\n",
    "1.什么是数据标准化？\n",
    "\n",
    "- 把数据缩放成平均值为0，方差为1的标准正态分布；\n",
    "\n",
    "2.为什么要进行数据标准化\n",
    "\n",
    "- 模型的表现可能会更好；\n",
    "\n",
    "- 模型中有多个特征，把这多个特征的数量级统一到一个量级上，避免数量级带来的影响；\n",
    "\n",
    "- 模型收敛得更快一些；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.18599891, -0.07412493, -0.96362411,  1.70487343,  0.81537425,\n",
       "       -0.29649973])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "X = np.array([0., 5., 1., 13., 9., 4.])\n",
    "scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "155px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
