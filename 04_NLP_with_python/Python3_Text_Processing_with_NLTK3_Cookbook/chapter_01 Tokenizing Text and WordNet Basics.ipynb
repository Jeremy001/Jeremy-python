{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter_01 Tokenizing Text and WordNet Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para1 = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "# sent_tokenize用来分句子\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(para1)\n",
    "# 我们看到这段话被分成了3句\n",
    "# sent_tokenize是怎么做到的呢？\n",
    "# 它用了nltk.tokenize.punkt模块，这个模块已经被训练了，会识别句子的开始和结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['大家好，我叫李白。这是我的好朋友，杜甫。我们都是唐朝著名的诗人，哈哈哈！']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para2 = '大家好，我叫李白。这是我的好朋友，杜甫。我们都是唐朝著名的诗人，哈哈哈！' \n",
    "sent_tokenize(para2)\n",
    "# 好坑，发现sent_tokenize对汉字段落的分句能力不高\n",
    "# 有哪些包可以用来处理中文文本分析？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果要分句的段落较长，那么直接把punkt模块下的pickle文件load进来，效率会更高\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize(para1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola amigo.', 'estoy bien.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果要分句的段落是其他语言，那么可以把其他语言的pickle文件load进来\n",
    "# 西班牙语\n",
    "para3 = 'hola amigo. estoy bien.'\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_tokenizer.tokenize(para3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenizing sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('hello world.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize调用了TreebankWordTokenizer这个类\n",
    "# 因此下面的代码也可以实现word tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('hello world.')\n",
    "# 原理：根据空格和标点符号来分词\n",
    "# 默认的，标点符号也分出来了\n",
    "# 根源：TokenizerI, 分出三类：PunktWordTokenizer、TreebankWordTokenizer、RegexpTokenizer\n",
    "# 从RegexpTokenizer又有两类：WordPuncktTokenizer、WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating contractions 对缩略词进行分词\n",
    "word_tokenize(\"can't\")\n",
    "# OMG!没有识别出是缩略词，这完全不可接受呀，怎么办？\n",
    "# 可以用regexp tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', \"'\", 't', 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"can't is a contraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing sentences using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用正则表达式会使分词变得复杂，效率降低，因此建议只有当之前的分词结果都不可接受时才使用\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面的代码是相同的结果\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple whitespace tokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本的格式不是传统的文章格式，比如对话，那么可以用自己训练好的tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "# 用PunktSentenceTokenizer训练\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?\\nAsian girl: Yeah, being angry!\\nWhite guy: Oh, that sounds good.\\n\\nGuy #1: So this Jack guy is basically the luckiest man in the world.\\nGuy #2: Why'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White guy: So, do you have any plans for this evening?',\n",
       " 'Asian girl: Yeah, being angry!',\n",
       " 'White guy: Oh, that sounds good.',\n",
       " 'Guy #1: So this Jack guy is basically the luckiest man in the world.',\n",
       " \"Guy #2: Why, because he's survived like 5 attempts on his life and it's not even noon?\",\n",
       " 'Guy #1: No; he could totally nail those two chicks.',\n",
       " 'Dad: Could you tell me where the auditorium is?',\n",
       " \"Security guy: It's on the second floor.\",\n",
       " \"Dad: Wait, you mean it's actually in the building?\",\n",
       " \"Girl: But, I mean, it's not like I ever plan on giving birth.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词，看结果\n",
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "sents1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用默认的sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White guy: So, do you have any plans for this evening?',\n",
       " 'Asian girl: Yeah, being angry!',\n",
       " 'White guy: Oh, that sounds good.',\n",
       " 'Guy #1: So this Jack guy is basically the luckiest man in the world.',\n",
       " \"Guy #2: Why, because he's survived like 5 attempts on his life and it's not even noon?\",\n",
       " 'Guy #1: No; he could totally nail those two chicks.',\n",
       " 'Dad: Could you tell me where the auditorium is?',\n",
       " \"Security guy: It's on the second floor.\",\n",
       " \"Dad: Wait, you mean it's actually in the building?\",\n",
       " \"Girl: But, I mean, it's not like I ever plan on giving birth.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents2[0:10]\n",
    "# 好像没什么差别？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents1[678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all theatrical.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents2[678]\n",
    "# 非也，发现训练的分词还是更有效，默认的把第二行也分一起了。\n",
    "# 训练的原理：利用非监督算法，学得句子的边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White guy: So, do you have any plans for this evening?',\n",
       " 'Asian girl: Yeah, being angry!',\n",
       " 'White guy: Oh, that sounds good.',\n",
       " 'Guy #1: So this Jack guy is basically the luckiest man in the world.',\n",
       " \"Guy #2: Why, because he's survived like 5 attempts on his life and it's not even noon?\",\n",
       " 'Guy #1: No; he could totally nail those two chicks.',\n",
       " 'Dad: Could you tell me where the auditorium is?',\n",
       " \"Security guy: It's on the second floor.\",\n",
       " \"Dad: Wait, you mean it's actually in the building?\",\n",
       " \"Girl: But, I mean, it's not like I ever plan on giving birth.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以从本地读取corpus用来训练\n",
    "with open('C:/Users/dakongyi/AppData/Roaming/nltk_data/corpora/webtext/overheard.txt', \n",
    "          encoding = 'ISO-8859-2') as f:\n",
    "    text = f.read()\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "sents1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents1[678]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopword的含义不用多解释\n",
    "# stopword放在nltk_data/corpora/stopwords/目录下\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各个语言的停止词文件\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 法语的停止词列表\n",
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking up synsets for a word in wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook.n.01'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a book of recipes and cooking directions'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cookbook.n.01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('cookbook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking can be a great art',\n",
       " 'people are needed who have experience in cookery',\n",
       " 'he left the preparation of meals to his wife']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同义词示例\n",
    "wordnet.synsets('cooking')[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('reference_book.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hypernyms：上义词\n",
    "# hyponyms：下义词\n",
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('annual.n.02'),\n",
       " Synset('atlas.n.02'),\n",
       " Synset('cookbook.n.01'),\n",
       " Synset('directory.n.01'),\n",
       " Synset('encyclopedia.n.01'),\n",
       " Synset('handbook.n.01'),\n",
       " Synset('instruction_book.n.01'),\n",
       " Synset('source_book.n.01'),\n",
       " Synset('wordbook.n.01')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 跟cookbook同一级的词都有哪些？\n",
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root_hypernyms是根上义词，最顶部的那个词\n",
    "syn.root_hypernyms()\n",
    "# cookbook的上义词是reference_book\n",
    "# reference_book的下义词有很多，比如annual、cookbook等\n",
    "# reference_book的root_hypernyms是entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('creation.n.02'),\n",
       "  Synset('product.n.02'),\n",
       "  Synset('work.n.02'),\n",
       "  Synset('publication.n.01'),\n",
       "  Synset('book.n.01'),\n",
       "  Synset('reference_book.n.01'),\n",
       "  Synset('cookbook.n.01')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以获取到整个路径\n",
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part of speech(POS)：词性\n",
    "# POS tag：词性标注\n",
    "# noun n\n",
    "# adjective a\n",
    "# adverb r\n",
    "# verb v\n",
    "len(wordnet.synsets('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets('great', pos = 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets('great', pos = 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('great.n.01')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('great', pos = 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up lemmas and synonyms in wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemma, is the caconical form or morphological form of a word\n",
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookery_book'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[1].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].synset() == lemmas[1].synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cookbook', 'cookery_book']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cookbook和cookery_book属于相同的synset(同义词集)\n",
    "# 因此两者的含义是相同的\n",
    "[lemma.name() for lemma in syn.lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 很多单词会有多个含义，因此就会有多个同义词集\n",
    "# 如果想要查找某个词的所有同义词，如下：\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('book'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "len(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'book',\n",
       " 'volume',\n",
       " 'record',\n",
       " 'record_book',\n",
       " 'book',\n",
       " 'script',\n",
       " 'book',\n",
       " 'playscript',\n",
       " 'ledger',\n",
       " 'leger',\n",
       " 'account_book',\n",
       " 'book_of_account',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'rule_book',\n",
       " 'Koran',\n",
       " 'Quran',\n",
       " \"al-Qur'an\",\n",
       " 'Book',\n",
       " 'Bible',\n",
       " 'Christian_Bible',\n",
       " 'Book',\n",
       " 'Good_Book',\n",
       " 'Holy_Scripture',\n",
       " 'Holy_Writ',\n",
       " 'Scripture',\n",
       " 'Word_of_God',\n",
       " 'Word',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'reserve',\n",
       " 'hold',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bible',\n",
       " 'Book',\n",
       " 'Christian_Bible',\n",
       " 'Good_Book',\n",
       " 'Holy_Scripture',\n",
       " 'Holy_Writ',\n",
       " 'Koran',\n",
       " 'Quran',\n",
       " 'Scripture',\n",
       " 'Word',\n",
       " 'Word_of_God',\n",
       " 'account_book',\n",
       " \"al-Qur'an\",\n",
       " 'book',\n",
       " 'book_of_account',\n",
       " 'hold',\n",
       " 'ledger',\n",
       " 'leger',\n",
       " 'playscript',\n",
       " 'record',\n",
       " 'record_book',\n",
       " 'reserve',\n",
       " 'rule_book',\n",
       " 'script',\n",
       " 'volume'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 发现虽然有38个，但是有重复的\n",
    "# 剔除掉重复的\n",
    "set(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating WordNet Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算同义词集之前的相似度\n",
    "from nltk.corpus import wordnet\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)\n",
    "# 发现两个同义词集的相似度很高\n",
    "# wup_similarity的全称是Wu-Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# wup_similarity的计算逻辑：\n",
    "# 根据各个同义词集在hypernym tree中所处在位置，计算相似性，相距越近，相似度越高\n",
    "# cookbook的上义词reference_book\n",
    "ref = cb.hypernyms()[0]\n",
    "print(cb.shortest_path_distance(ref))\n",
    "print(ib.shortest_path_distance(ref))\n",
    "print(cb.shortest_path_distance(ib))\n",
    "# 发现他们的上义词是相同的reference_book，都有一步之遥，距离彼此2步之遥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对比不相似的两个词，看看wup_similarity是多少？\n",
    "dog = wordnet.synsets('dog')[0]\n",
    "dog.wup_similarity(cb)\n",
    "# 由于两个词往上追溯还是有相同的上义词，因此还有38%的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('whole.n.02')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看两个词共享的上义词\n",
    "sorted(dog.common_hypernyms(cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对比动词\n",
    "cook = wordnet.synset('cook.v.01')\n",
    "bake = wordnet.synset('bake.v.02')\n",
    "cook.wup_similarity(bake)\n",
    "# 由于动词不像名词一样经常有相同的上义词，因此动词之间的相似度更小，甚至较难计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.07142857142857142\n",
      "2.538973871058276\n",
      "0.9985288301111273\n"
     ]
    }
   ],
   "source": [
    "# path similarity and LCH similarity\n",
    "print(cb.path_similarity(ib))\n",
    "print(cb.path_similarity(dog))\n",
    "print(cb.lch_similarity(ib))\n",
    "print(cb.lch_similarity(dog))\n",
    "# 发现这两种计算结果都比较不太好理解，不像wup_similarity是0-1范围的，很容易比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering word collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collocations是一对词，这对词经常一起出现的，比如United States\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n",
    "# 发现前4个没啥用，因为这些体现的是语法的配合使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过设置停止词来剔除语法的因素\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w:len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 'term', 'relationship')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 除了一对词，还有三个词\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "words = [w.lower() for w in webtext.words('singles.txt')]\n",
    "tcf = TrigramCollocationFinder.from_words(words)\n",
    "tcf.apply_word_filter(filter_stops)\n",
    "# 选择出现次数大于等于3次的collocations\n",
    "tcf.apply_freq_filter(3)\n",
    "tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)\n",
    "# 发现只有一组"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
